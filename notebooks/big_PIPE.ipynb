{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7731aeb0-5759-4b45-9aa8-b0d37f408d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-14T05:15:35.891011Z",
     "iopub.status.busy": "2024-02-14T05:15:35.889980Z",
     "iopub.status.idle": "2024-02-14T05:15:35.996512Z",
     "shell.execute_reply": "2024-02-14T05:15:35.994504Z",
     "shell.execute_reply.started": "2024-02-14T05:15:35.891011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503213f5-72c7-4188-b898-7434f1d62608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-14T05:31:29.863042Z",
     "iopub.status.busy": "2024-02-14T05:31:29.861008Z",
     "iopub.status.idle": "2024-02-14T05:31:32.445426Z",
     "shell.execute_reply": "2024-02-14T05:31:32.444424Z",
     "shell.execute_reply.started": "2024-02-14T05:31:29.862041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while fetching content from http://arxiv.org/abs/quant-ph/0302169v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/1212.4177v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/1504.03207v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/2208.08064v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/cond-mat/0601285v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/0811.2516v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/1807.11019v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/quant-ph/0201082v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/quant-ph/0309066v1.pdf: EOF marker not found\n",
      "Error occurred while fetching content from http://arxiv.org/abs/quant-ph/0504224v1.pdf: EOF marker not found\n",
      "                                           URL  Text\n",
      "0  http://arxiv.org/abs/quant-ph/0302169v1.pdf  None\n",
      "1         http://arxiv.org/abs/1212.4177v1.pdf  None\n",
      "2        http://arxiv.org/abs/1504.03207v1.pdf  None\n",
      "3        http://arxiv.org/abs/2208.08064v1.pdf  None\n",
      "4  http://arxiv.org/abs/cond-mat/0601285v1.pdf  None\n",
      "5         http://arxiv.org/abs/0811.2516v1.pdf  None\n",
      "6        http://arxiv.org/abs/1807.11019v1.pdf  None\n",
      "7  http://arxiv.org/abs/quant-ph/0201082v1.pdf  None\n",
      "8  http://arxiv.org/abs/quant-ph/0309066v1.pdf  None\n",
      "9  http://arxiv.org/abs/quant-ph/0504224v1.pdf  None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_arxiv_data(search_query, max_results=10, start_date=None, end_date=None, primary_category=None, categories=None):\n",
    "    # Define the API endpoint URL for searching articles\n",
    "    api_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    # Define additional parameters like start and max_results\n",
    "    params = {\n",
    "        \"search_query\": search_query,\n",
    "        \"start\": 0,  # Start index of results\n",
    "        \"max_results\": max_results,  # Maximum number of results to retrieve\n",
    "    }\n",
    "\n",
    "    # Add start and end date parameters if provided\n",
    "    if start_date is not None:\n",
    "        params[\"start_date\"] = start_date\n",
    "    if end_date is not None:\n",
    "        params[\"end_date\"] = end_date\n",
    "\n",
    "    # Add primary category and categories parameters if provided\n",
    "    if primary_category is not None:\n",
    "        params[\"cat\"] = primary_category\n",
    "    if categories is not None:\n",
    "        params[\"categories\"] = categories\n",
    "\n",
    "    # Send an HTTP GET request to the arXiv API\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "\n",
    "        # Create a list to store article information\n",
    "        articles = []\n",
    "\n",
    "        # Extract article information from the parsed content\n",
    "        entries = soup.find_all(\"entry\")\n",
    "        for entry in entries:\n",
    "            title = entry.find(\"title\").text\n",
    "            paper_id = entry.find(\"id\").text\n",
    "            published = entry.find(\"published\").text\n",
    "            updated = entry.find(\"updated\").text\n",
    "            summary = entry.find(\"summary\").text\n",
    "            author = [author.text for author in entry.find_all(\"author\")]\n",
    "            comments = entry.find(\"arxiv:comment\").text if entry.find(\"arxiv:comment\") else \"\"\n",
    "            journal_ref = entry.find(\"arxiv:journal_ref\").text if entry.find(\"arxiv:journal_ref\") else \"\"\n",
    "            link = entry.find(\"link\", title=\"pdf\")[\"href\"] if entry.find(\"link\", title=\"pdf\") else \"\"\n",
    "            primary_category = entry.find(\"arxiv:primary_category\")[\"term\"] if entry.find(\"arxiv:primary_category\") else \"\"\n",
    "            categories = [cat[\"term\"] for cat in entry.find_all(\"category\")]\n",
    "            doi = entry.find(\"arxiv:doi\").text if entry.find(\"arxiv:doi\") else \"\"\n",
    "            license = entry.find(\"arxiv:license\")[\"type\"] if entry.find(\"arxiv:license\") else \"\"\n",
    "            affiliation = [aff.text for aff in entry.find_all(\"arxiv:affiliation\")]\n",
    "\n",
    "            # Append article information to the list\n",
    "            articles.append({\n",
    "                \"Title\": title,\n",
    "                \"ID\": paper_id,\n",
    "                \"Published\": published,\n",
    "                \"Updated\": updated,\n",
    "                \"Summary\": summary,\n",
    "                \"Author\": author,\n",
    "                \"Comments\": comments,\n",
    "                \"Journal_Ref\": journal_ref,\n",
    "                \"Link\": link,\n",
    "                \"Primary_Category\": primary_category,\n",
    "                \"Categories\": categories,\n",
    "                \"DOI\": doi,\n",
    "                \"License\": license,\n",
    "                \"Affiliation\": affiliation,\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the list of articles\n",
    "        df = pd.DataFrame(articles)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from arXiv API\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(url):\n",
    "    try:\n",
    "        # Access the PDF from the URL\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Read the PDF content\n",
    "            pdf_content = io.BytesIO(response.content)\n",
    "            \n",
    "            # Extract text from the PDF\n",
    "            reader = PdfReader(pdf_content)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            return text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching content from {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fetch data from arXiv and store it in a DataFrame\n",
    "df_arxiv = fetch_arxiv_data(search_query=\"quantum physics\", max_results=10, start_date=\"2023-01-01\", end_date=\"2023-12-31\", primary_category=\"quant-ph\", categories=[\"quant-ph\", \"cond-mat\"])\n",
    "\n",
    "# Generate new URLs with \".pdf\" extension appended to the IDs\n",
    "new_urls = [f\"{id}.pdf\" for id in df_arxiv['ID'].unique()]\n",
    "\n",
    "# Extract text from the PDFs corresponding to the new URLs\n",
    "pdf_contents = [extract_text_from_pdf(url) for url in new_urls]\n",
    "\n",
    "# Create a new DataFrame with the URLs and extracted text\n",
    "df_pdf_contents = pd.DataFrame({'URL': new_urls, 'Text': pdf_contents})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_pdf_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d515fb-70b7-490d-b55b-1e3897d72ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
